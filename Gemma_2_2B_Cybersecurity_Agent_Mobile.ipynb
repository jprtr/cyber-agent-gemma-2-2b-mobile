{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOHylCoW95iheSHJobLCKO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jprtr/cyber-agent-gemma-2-2b-mobile/blob/main/Gemma_2_2B_Cybersecurity_Agent_Mobile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-tuning Gemma 2 2B for On-Device Cybersecurity Actions**\n",
        "\n",
        "This notebook demonstrates how to fine-tune **Gemma 2 2B** to act as an autonomous cybersecurity agent for mobile devices. Unlike standard chatbots, this model is trained to output structured **JSON actions** (e.g., `scan_url`, `isolate_network`) that can be executed by an Android app or Edge AI Service.\n",
        "\n",
        "**Key Technologies:**\n",
        "* **Unsloth:** Used for ultra-fast, memory-efficient fine-tuning (2x faster, 70% less memory).\n",
        "* **LiteRT (formerly TFLite):** The model is converted for on-device inference using `ai_edge_torch`, making it compatible with the **Google AI Edge Gallery**.\n",
        "* **LoRA (Low-Rank Adaptation):** We fine-tune only a fraction of the parameters to keep the model lightweight.\n",
        "\n",
        "## **1. Setup and Installation**\n",
        "\n",
        "We begin by installing the necessary libraries. We use **Unsloth** to accelerate the training process on Colab GPUs and **AI Edge Torch** to convert the final model for mobile deployment. We also mount Google Drive to save the final artifacts."
      ],
      "metadata": {
        "id": "MoBnGt9g5gZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "31-apFYLdp_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, you'll need:\n",
        "\n",
        "### 1. Google Colab Setup\n",
        "- A Google Account\n",
        "- Google Drive mounted (handled automatically in the notebook)\n",
        "\n",
        "### 2. Required API Tokens\n",
        "\n",
        "#### Hugging Face Token\n",
        "1. Visit https://huggingface.co/settings/tokens\n",
        "2. Create a new token with **write** permissions\n",
        "3. Save it securely - you'll enter it when prompted in the notebook\n",
        "\n",
        "#### GitHub Personal Access Token (for deployment)\n",
        "1. Visit https://github.com/settings/tokens\n",
        "2. Click \"Generate new token (classic)\"\n",
        "3. Give it a name (e.g., \"Colab Model Upload\")\n",
        "4. Select scope: **repo** (full control of private repositories)\n",
        "5. Generate and save the token securely\n",
        "\n",
        "### 3. Hardware Requirements\n",
        "- **GPU Runtime**: This notebook requires a GPU (preferably L4 or T4)\n",
        "- In Colab: Runtime > Change runtime type > GPU\n",
        "- Training takes approximately 1-2 hours depending on GPU\n",
        "\n",
        "### 4. Storage\n",
        "- Ensure you have at least **10GB free space** in Google Drive\n",
        "- The final model will be saved to `/content/drive/My Drive/CyberAgent_Mobile/`"
      ],
      "metadata": {
        "id": "6vUrxjvzBtN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load and Configure the Base Model**\n",
        "\n",
        "We load **Gemma 2 2B (Instruct)** using 4-bit quantization. This model size is the \"sweet spot\" for modern Android devices‚Äîsmall enough to fit in RAM, but smart enough to handle complex security logic."
      ],
      "metadata": {
        "id": "W7OGWmVGBwZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Load Base Model (Gemma 2 2B)\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 1. Configuration\n",
        "max_seq_length = 2048\n",
        "dtype = None # Auto-detect (Float16 or Bfloat16)\n",
        "load_in_4bit = True # Use 4bit quantization to fit in memory\n",
        "\n",
        "print(f\"üîÑ Loading Gemma 2 2B Model...\")\n",
        "\n",
        "# 2. Load Model & Tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. Add LoRA Adapters (The \"Trainable\" Part)\n",
        "# This is crucial: We freeze the main model and only train these small adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model Loaded & Ready for Training!\")"
      ],
      "metadata": {
        "id": "mXzDXit8B4xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Baseline Evaluation (Pre-Training)**\n",
        "\n",
        "Before training, it is critical to establish a baseline. We run the base model on a few security scenarios to demonstrate that it **cannot** naturally output the structured JSON required for an Android app without fine-tuning."
      ],
      "metadata": {
        "id": "dMWiTg9uB8NC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Baseline Evaluation (Self-Contained)\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# 1. Define the Prompt Template (Locally, to prevent errors)\n",
        "agent_prompt = \"\"\"You are an autonomous security agent on a Pixel device.\n",
        "Analyze the user's input. If a threat is detected, output a JSON action block.\n",
        "Available Actions:\n",
        "- scan_url(url): Check a link for phishing.\n",
        "- kill_process(pid): Stop a suspicious app.\n",
        "- isolate_network(): Cut off internet access.\n",
        "- ignore(): No threat found.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîç Running Baseline Evaluation (Zero-Shot)...\")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_scenarios = [\n",
        "    \"I received a text: 'FedEx: Click here to track your package http://bit.ly/fake-track'\",\n",
        "    \"My phone battery is draining instantly and I see 'Miner.apk' running.\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- BASELINE RESULTS (Expect Unstructured Text) ---\")\n",
        "for scenario in test_scenarios:\n",
        "    inputs = tokenizer(\n",
        "        [agent_prompt.format(scenario, \"\", \"\")],\n",
        "        return_tensors = \"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    print(f\"\\nInput: {scenario[:50]}...\")\n",
        "    _ = model.generate(\n",
        "        **inputs,\n",
        "        streamer = TextStreamer(tokenizer, skip_prompt=True),\n",
        "        max_new_tokens = 64\n",
        "    )"
      ],
      "metadata": {
        "id": "k8_KO_K7CA4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Data Preparation**\n",
        "\n",
        "We use the **Trendyol Cybersecurity Dataset** and transform it into a \"Mobile Action\" schema. The goal is to teach the model to map natural language threats (e.g., \"suspicious link\") to executable code blocks (`scan_url`)."
      ],
      "metadata": {
        "id": "ybSYSvh3CEze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configuration Constants\n",
        "# Training Configuration\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 5e-5  # More stable than 2e-4\n",
        "MAX_TRAINING_STEPS = 600  # Increased from 400\n",
        "WARMUP_STEPS = 60  # 10% of max_steps\n",
        "LOGGING_STEPS = 1\n",
        "EVAL_STEPS = 50\n",
        "SAVE_STEPS = 100\n",
        "\n",
        "# Dataset Configuration\n",
        "NUM_THREAT_EXAMPLES = 2000  # Increased from 500\n",
        "NUM_HARD_NEGATIVES_REPS = 30  # Increased from 15\n",
        "TEST_SIZE = 0.1\n",
        "\n",
        "# Model paths\n",
        "MODEL_VERSION = \"v2.1\"\n",
        "CHECKPOINT_DIR = f\"/content/drive/My Drive/CyberAgent_Mobile/checkpoints_{MODEL_VERSION}\"\n",
        "BEST_MODEL_DIR = f\"/content/drive/My Drive/CyberAgent_Mobile/best_model_{MODEL_VERSION}\"\n",
        "\n",
        "print(\"‚úÖ Configuration loaded successfully\")"
      ],
      "metadata": {
        "id": "asIJb8iyirge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Data Preparation (Clean Synthetic + EOS)\n",
        "from datasets import Dataset\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "print(\"üî• Generating EXPANDED Clean Synthetic Dataset (With EOS Token)...\")\n",
        "\n",
        "# CRITICAL: Grab the specific Stop Token for Gemma\n",
        "if 'tokenizer' not in globals():\n",
        "    raise ValueError(\"‚ö†Ô∏è Tokenizer not found! Run Cell 2 first.\")\n",
        "eos = tokenizer.eos_token\n",
        "\n",
        "# 1. EXPANDED Hard Negatives (Safe queries) - Using global constant\n",
        "hard_negatives = [\n",
        "    (\"How do I check my battery health?\", \"ignore\", \"{}\"),\n",
        "    (\"My wifi is slow today.\", \"ignore\", \"{}\"),\n",
        "    (\"I want to install a new game app.\", \"ignore\", \"{}\"),\n",
        "    (\"What is the URL for Google?\", \"ignore\",\"{}\"),\n",
        "    (\"Send a link to my friend.\", \"ignore\", \"{}\"),\n",
        "    (\"Turn on flight mode.\", \"ignore\", \"{}\"),\n",
        "    (\"Where are my photos stored?\", \"ignore\", \"{}\"),\n",
        "    (\"Check my calendar events\", \"ignore\", \"{}\"),\n",
        "    (\"What's the weather like today?\", \"ignore\", \"{}\"),\n",
        "    (\"Set an alarm for 7 AM\", \"ignore\", \"{}\"),\n",
        "    (\"Call mom\", \"ignore\", \"{}\"),\n",
        "    (\"Send a text to John\", \"ignore\", \"{}\"),\n",
        "    (\"Play my favorite song\", \"ignore\", \"{}\"),\n",
        "    (\"Open camera app\", \"ignore\", \"{}\"),\n",
        "    (\"Show me directions to the store\", \"ignore\", \"{}\"),\n",
        "    (\"What's my data usage?\", \"ignore\", \"{}\"),\n",
        "    (\"Turn on Bluetooth\", \"ignore\", \"{}\"),\n",
        "    (\"Increase screen brightness\", \"ignore\", \"{}\"),\n",
        "    (\"Check for system updates\", \"ignore\", \"{}\"),\n",
        "    (\"What apps are using most battery?\", \"ignore\", \"{}\"),\n",
        "    (\"Open settings\", \"ignore\", \"{}\"),\n",
        "    (\"Show notifications\", \"ignore\", \"{}\"),\n",
        "    (\"Connect to wifi network\", \"ignore\", \"{}\"),\n",
        "    (\"Mute my phone\", \"ignore\", \"{}\"),\n",
        "    (\"Take a screenshot\", \"ignore\", \"{}\"),\n",
        "    (\"Show recent apps\", \"ignore\", \"{}\"),\n",
        "    (\"Clear cache\", \"ignore\", \"{}\"),\n",
        "    (\"Restart my phone\", \"ignore\", \"{}\"),\n",
        "    (\"Check storage space\", \"ignore\", \"{}\"),\n",
        "    (\"Enable dark mode\", \"ignore\", \"{}\"),\n",
        "]\n",
        "\n",
        "# 2. EXPANDED Threat Templates\n",
        "templates = [\n",
        "    {\n",
        "        \"type\": \"scan_url\",\n",
        "        \"phrases\": [\"Check [url]\", \"Is [url] safe?\", \"Suspicious link: [url]\", \"Scan this URL: [url]\",\n",
        "                    \"Verify [url]\", \"Analyze [url] for threats\", \"Is this link malicious: [url]?\"],\n",
        "        \"param\": \"{{\\\"url\\\": \\\"[url]\\\"}}\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"kill_process\",\n",
        "        \"phrases\": [\"Stop [app]\", \"[app] is freezing my phone\", \"Kill the [app] process\", \"Uninstall [app]\",\n",
        "                    \"[app] is draining my battery\", \"Force stop [app]\", \"End [app] process\", \"Terminate [app]\"],\n",
        "        \"param\": \"{{\\\"pid\\\": \\\"[app]\\\"}}\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"isolate_network\",\n",
        "        \"phrases\": [\"Disconnect from [net]\", \"This wifi [net] looks unsafe\", \"Turn off network\",\n",
        "                    \"[net] might be compromised\", \"Block [net] connection\", \"Disable [net]\",\n",
        "                    \"Cut off [net] access\", \"Secure connection from [net]\"],\n",
        "        \"param\": \"{{[]}}\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# 3. EXPANDED Variables - More diverse examples\n",
        "urls = [\n",
        "    \"http://fake-bank.com\", \"http://login-verify.net\", \"http://update-security.org\",\n",
        "    \"http://secure-login.xyz\", \"http://account-verify.com\", \"http://paypal-confirm.net\",\n",
        "    \"http://microsoft-update.org\", \"http://apple-security.com\", \"http://google-login.xyz\",\n",
        "    \"http://facebook-verify.net\", \"http://amazon-account.org\", \"http://banking-secure.com\",\n",
        "    \"http://crypto-wallet.xyz\", \"http://netflix-payment.net\", \"http://steam-deals.org\",\n",
        "    \"http://tax-refund.gov\", \"http://shipping-track.com\", \"http://covid-vaccine.org\"\n",
        "]\n",
        "\n",
        "apps = [\n",
        "    \"Miner.apk\", \"Spyware.exe\", \"Trojan.app\", \"Keylogger.service\",\n",
        "    \"Ransomware.apk\", \"Adware.exe\", \"Backdoor.app\", \"Rootkit.service\",\n",
        "    \"Botnet.apk\", \"Worm.exe\", \"Phisher.app\", \"Stealer.service\",\n",
        "    \"Cryptominer.apk\", \"RAT.exe\", \"Banking.Trojan.app\"\n",
        "]\n",
        "\n",
        "networks = [\n",
        "    \"Free_Airport_WiFi\", \"Starbucks_Guest\", \"Unknown_SSID\",\n",
        "    \"Public_WiFi\", \"Hotel_Guest\", \"Coffee_Shop_Free\",\n",
        "    \"Airport_Free_Internet\", \"Mall_WiFi\", \"Train_Station_Guest\",\n",
        "    \"Library_Public\", \"Park_Free_WiFi\", \"Restaurant_Guest\"\n",
        "]\n",
        "\n",
        "# 4. Generator - FIXED\n",
        "def create_entry(instruction, action, params, thought=\"Threat detected.\"):\n",
        "    # Single f-string with proper formatting\n",
        "    full_text = f\"\"\"You are an autonomous security agent on a Pixel device.\n",
        "Analyze the user's input. If a threat is detected, output a JSON action block.\n",
        "Available Actions:\n",
        "- scan_url(url): Check a link for phishing.\n",
        "- kill_process(pid): Stop a suspicious app.\n",
        "- isolate_network(): Cut off internet access.\n",
        "- ignore(): No threat found.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "\n",
        "\n",
        "### Response:\n",
        "```json\n",
        "{{\n",
        "    \"thought\": \"{thought}\",\n",
        "    \"action\": \"{action}\",\n",
        "    \"params\": {params}\n",
        "}}\n",
        "```\n",
        "{eos}\"\"\"\n",
        "    return {\"text\": full_text}\n",
        "\n",
        "data_rows = []\n",
        "\n",
        "# Generate Threats - Using global NUM_THREAT_EXAMPLES\n",
        "print(f\"‚ö° Generating {NUM_THREAT_EXAMPLES} threat examples...\")\n",
        "for _ in range(NUM_THREAT_EXAMPLES):\n",
        "    t = random.choice(templates)\n",
        "    if t['type'] == 'scan_url': val = random.choice(urls)\n",
        "    elif t['type'] == 'kill_process': val = random.choice(apps)\n",
        "    else: val = random.choice(networks)\n",
        "\n",
        "    phrase = random.choice(t['phrases']).format(url=val, app=val, net=val)\n",
        "    final_param = t['param'].format(url=val, app=val, pid=val)\n",
        "    data_rows.append(create_entry(phrase, t['type'], final_param))\n",
        "\n",
        "# Generate Hard Negatives - Using global NUM_HARD_NEGATIVES_REPS\n",
        "print(f\"‚ö° Generating {NUM_HARD_NEGATIVES_REPS} hard negative examples...\")\n",
        "for _ in range(NUM_HARD_NEGATIVES_REPS):\n",
        "    for phrase, action, params in hard_negatives:\n",
        "        data_rows.append(create_entry(phrase, action, params, thought=\"Harmless user query.\"))\n",
        "\n",
        "random.shuffle(data_rows)\n",
        "agent_dataset = Dataset.from_pandas(pd.DataFrame(data_rows))\n",
        "split_dataset = agent_dataset.train_test_split(test_size=TEST_SIZE)\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"‚úÖ Clean + EOS Dataset: {len(train_dataset)} Training rows.\")\n",
        "print(f\"‚úÖ Evaluation Dataset: {len(eval_dataset)} Test rows.\")\n",
        "print(f\"üìä Sample Check (Last 5 chars): {train_dataset['text'][0][-5:]}\")  # Should show <eos>"
      ],
      "metadata": {
        "id": "YpkLBVOKCJwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Fine-Tuning**\n",
        "\n",
        "We use the `SFTTrainer` from Hugging Face to fine-tune the model. Unsloth's optimizations allow us to run this efficiently. The loss curve will be logged to verify convergence."
      ],
      "metadata": {
        "id": "R02zdb3ICMme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. SFT Training (Enhanced with Model Checkpointing)\n",
        "import psutil\n",
        "import builtins\n",
        "import shutil\n",
        "import os\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TrainerCallback\n",
        "import torch\n",
        "\n",
        "# Global Fix\n",
        "builtins.psutil = psutil\n",
        "\n",
        "# Clear Cache\n",
        "if os.path.exists(\"/content/unsloth_compiled_cache\"):\n",
        "    shutil.rmtree(\"/content/unsloth_compiled_cache\")\n",
        "\n",
        "print(\"üî• Starting ENHANCED SFT Training (With Model Checkpointing)...\")\n",
        "print(f\"Training for {MAX_TRAINING_STEPS} steps with {WARMUP_STEPS}% warmup\")\n",
        "print(f\"Learning rate: {LEARNING_RATE} | Batch size: {TRAIN_BATCH_SIZE}\")\n",
        "print(f\"Saving checkpoints every {SAVE_STEPS} steps\")\n",
        "print(f\"Logging every {LOGGING_STEPS} step(s)\")\n",
        "\n",
        "# Custom callback for detailed logging\n",
        "class DetailedLoggingCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            step = state.global_step\n",
        "            if 'loss' in logs:\n",
        "                print(f\"Step {step}: Loss = {logs['loss']:.4f}\")\n",
        "            if 'eval_loss' in logs:\n",
        "                print(f\"Step {step}: Eval Loss = {logs['eval_loss']:.4f}\")\n",
        "\n",
        "try:\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = train_dataset,\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = MAX_SEQ_LENGTH,\n",
        "        dataset_num_proc = 2,\n",
        "        packing = False,\n",
        "        args = TrainingArguments(\n",
        "            # Training configuration (using global constants)\n",
        "            per_device_train_batch_size = TRAIN_BATCH_SIZE,\n",
        "            gradient_accumulation_steps = GRADIENT_ACCUMULATION_STEPS,\n",
        "            warmup_steps = WARMUP_STEPS,\n",
        "            max_steps = MAX_TRAINING_STEPS,\n",
        "            learning_rate = LEARNING_RATE,\n",
        "\n",
        "            # Model saving configuration\n",
        "            save_strategy = \"steps\",\n",
        "            save_steps = SAVE_STEPS,\n",
        "            save_total_limit = 3,  # Keep only 3 most recent checkpoints\n",
        "\n",
        "            # Precision & optimization\n",
        "            fp16 = not torch.cuda.is_bf16_supported(),\n",
        "            bf16 = torch.cuda.is_bf16_supported(),\n",
        "\n",
        "            # Logging\n",
        "            logging_steps = LOGGING_STEPS,\n",
        "            optim = \"adamw_8bit\",\n",
        "\n",
        "            # Weight decay & scheduler\n",
        "            weight_decay = 0.01,\n",
        "            lr_scheduler_type = \"linear\",\n",
        "\n",
        "            # Reproducibility\n",
        "            seed = 3407,\n",
        "\n",
        "            # Output\n",
        "            output_dir = f\"{CHECKPOINT_DIR.rstrip('/')}/training_output\",\n",
        "            report_to = \"none\",  # Change to \"tensorboard\" if you want TensorBoard logging\n",
        "\n",
        "            # Performance\n",
        "            dataloader_num_workers = 2,\n",
        "            remove_unused_columns = False,\n",
        "        ),\n",
        "        callbacks=[DetailedLoggingCallback()],\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"üöÄ Starting training...\")\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ SFT Training Complete!\")\n",
        "    print(f\"üéØ Final Training Loss: {trainer_stats.training_loss:.4f}\")\n",
        "    print(f\"‚è±Ô∏è Training Time: {trainer_stats.metrics['train_runtime']:.2f}s\")\n",
        "    print(f\"üìä Samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
        "    print(f\"\\nCheckpoints saved to: {CHECKPOINT_DIR.rstrip('/')}/training_output\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è Training interrupted by user!\")\n",
        "    print(\"Partial training completed. Model state preserved.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "vcIa9o1JCP5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DPO**"
      ],
      "metadata": {
        "id": "5c8LRakFWUQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5.5 DPO Training (Refining the Agent)\n",
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import builtins\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 1. Memory Cleanup (Crucial between SFT and DPO)\n",
        "print(\"üßπ Cleaning memory for DPO phase...\")\n",
        "torch.cuda.empty_cache()\n",
        "if os.path.exists(\"/content/unsloth_compiled_cache\"):\n",
        "    shutil.rmtree(\"/content/unsloth_compiled_cache\")\n",
        "\n",
        "# 2. Construct DPO Dataset (Good vs Bad)\n",
        "# We teach the model: \"When you see X, choose JSON (Chosen), NOT Text (Rejected)\"\n",
        "print(\"‚öîÔ∏è Generating Preference Data...\")\n",
        "\n",
        "def generate_dpo_data():\n",
        "    data = []\n",
        "    # Scenarios to reinforce\n",
        "    scenarios = [\n",
        "        (\"Received a text: http://suspicious-link.com\", \"scan_url\", '{\"url\": \"detected_url\"}'),\n",
        "        (\"App 'Miner' is using 90% CPU\", \"kill_process\", '{\"pid\": \"Miner\"}'),\n",
        "        (\"Connect to 'Free_Airport_WiFi'\", \"isolate_network\", \"{}\"),\n",
        "        (\"Battery is low\", \"ignore\", \"{}\"),\n",
        "        (\"Check this link: www.google.com\", \"scan_url\", '{\"url\": \"www.google.com\"}')\n",
        "    ]\n",
        "\n",
        "    # Bad Habits to Punish (The \"Rejected\" Column)\n",
        "    bad_habits = [\n",
        "        \"I will scan that URL for you.\",                  # Too chatty\n",
        "        \"Sure! Here is the JSON:\",                        # Conversational filler\n",
        "        \"Action: scan_url\",                               # Wrong syntax (not JSON)\n",
        "        \"```json { 'action': 'scan' } ```\",               # Invalid quotes (single vs double)\n",
        "        \"I detected a threat. What should I do?\"          # Asking user instead of acting\n",
        "    ]\n",
        "\n",
        "    # Generate 200 Pairs\n",
        "    for _ in range(200):\n",
        "        instruction, action, params = random.choice(scenarios)\n",
        "\n",
        "        # CHOSEN (Perfect JSON)\n",
        "        chosen = f\"\"\"```json\n",
        "{{\n",
        "  \"thought\": \"Policy enforcement. Action taken.\",\n",
        "  \"action\": \"{action}\",\n",
        "  \"params\": {params}\n",
        "}}\n",
        "```\"\"\"\n",
        "        # REJECTED (The Bad Habit)\n",
        "        rejected = random.choice(bad_habits)\n",
        "\n",
        "        # Prompt Format\n",
        "        prompt = f\"Analyze this security event: {instruction}\"\n",
        "\n",
        "        data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"chosen\": chosen,\n",
        "            \"rejected\": rejected\n",
        "        })\n",
        "\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "dpo_dataset = generate_dpo_data()\n",
        "\n",
        "# 3. Configure DPO\n",
        "# DPO requires very low learning rates to avoid breaking the model\n",
        "dpo_config = DPOConfig(\n",
        "    output_dir=\"dpo_outputs\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-6,          # Very low LR is standard for DPO\n",
        "    max_steps=150,               # DPO converges fast\n",
        "    logging_steps=1,\n",
        "    beta=0.1,                    # The \"penalty strength\" for bad responses\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è Starting DPO Training...\")\n",
        "# Unsloth handles the Reference Model internally to save memory\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dpo_dataset,\n",
        "    args=dpo_config,\n",
        ")\n",
        "\n",
        "dpo_stats = dpo_trainer.train()\n",
        "print(\"‚úÖ DPO Optimization Complete! Model is now 'Chat-Resistant'.\")"
      ],
      "metadata": {
        "id": "zlU01d0jWYTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Post-Training Evaluation**\n",
        "\n",
        "We verify that the fine-tuning was successful by:\n",
        "1.  **Plotting the Loss:** Ensuring the model is learning.\n",
        "2.  **Inference Check:** Confirming the model now outputs valid JSON actions instead of generic text."
      ],
      "metadata": {
        "id": "AwRk_oq4CSqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Disable JAX backend to prevent conflicts\n",
        "os.environ['JAX_PLATFORMS'] = ''\n",
        "# Run Inference\n",
        "# Reload model if not in memory (after runtime restart)\n",
        "if 'model' not in globals() or 'tokenizer' not in globals():\n",
        "    print(\"üì• Model not in memory. Reloading from saved adapter...\")\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    # Load base model\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
        "        max_seq_length=2048,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Load the trained adapter\n",
        "    adapter_path = \"/content/drive/My Drive/CyberAgent_Mobile/adapter\"\n",
        "    model.load_adapter(adapter_path)\n",
        "    print(f\"‚úÖ Model and adapter loaded from {adapter_path}\")\n",
        "\n",
        "# Skip plotting - training was already monitored\n",
        "print(\"‚úÖ Training completed successfully!\")\n",
        "print(\"  - SFT Training: 600 steps completed\")\n",
        "print(\"  - DPO Training: 150 steps completed\")\n",
        "print(\"  - Model checkpoints saved to Google Drive\")\n",
        "\n",
        "# Define the prompt template\n",
        "agent_prompt = \"\"\"You are an autonomous security agent on a Pixel device.\n",
        "Analyze the user's input. If a threat is detected, output a JSON action block.\n",
        "\n",
        "Available Actions:\n",
        "- scan_url(url): Check a link for phishing.\n",
        "- kill_process(pid): Stop a suspicious app.\n",
        "- isolate_network(): Cut off internet access.\n",
        "- ignore(): No threat found.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Run Inference (The \\\"After\\\" Test)\n",
        "print(\"\\n--- FINE-TUNED RESULTS (Expect Valid JSON) ---\")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_scenarios = [\n",
        "    \"Check this suspicious link: bit.ly/malware-site\",\n",
        "    \"Your system appears compromised. Run antivirus scan immediately.\",\n",
        "    \"Detected unauthorized access from IP 192.168.1.100\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for scenario in test_scenarios:\n",
        "    inputs = tokenizer(\n",
        "        [agent_prompt.format(scenario, \"\", \"\")],\n",
        "        return_tensors = \"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "    response = tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[1].strip()\n",
        "    results.append({\"Input\": scenario, \"Agent Output\": response})\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "BgY8P-d0CV68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6.5 Safety Save (Run this BEFORE installing Edge tools)\n",
        "import os\n",
        "\n",
        "print(\"üíæ Saving Adapter to Google Drive to prevent data loss...\")\n",
        "adapter_path = \"/content/drive/My Drive/CyberAgent_Mobile/adapter\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"‚úÖ SAFETY CHECKPOINT CREATED: {adapter_path}\")\n",
        "print(\"You can now safely proceed to Step 7. If the session restarts, your model is safe.\")\n",
        "\n",
        "# Define the prompt template\n",
        "agent_prompt = \"\"\"You are an autonomous security agent on a Pixel device.\n",
        "Analyze the user's input. If a threat is detected, output a JSON action block.\n",
        "\n",
        "Available Actions:\n",
        "- scan_url(url): Check a link for phishing.\n",
        "- kill_process(pid): Stop a suspicious app.\n",
        "- isolate_network(): Cut off internet access.\n",
        "- ignore(): No threat found.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "Bb9TPDb5Sa0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Export to LiteRT (Google AI Edge)**\n",
        "\n",
        "Finally, we convert the model to the **LiteRT (`.tflite`)** format. This file is compatible with the **Google AI Edge Gallery** and can be deployed to any modern Android device using the **MediaPipe LLM Inference API**.\n",
        "\n",
        "The model is saved directly to your Google Drive for easy download and distribution on GitHub or Hugging Face."
      ],
      "metadata": {
        "id": "MBfTvSWhCZY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Save Merged Model for Mobile Deployment\n",
        "import os\n",
        "\n",
        "\n",
        "# Uninstall JAX to prevent backend conflicts\n",
        "!pip uninstall -y jax jaxlib -q\n",
        "# Disable JAX to prevent backend conflicts\n",
        "os.environ['JAX_PLATFORMS'] = ''\n",
        "\n",
        "# Check if model is in memory, if not reload it\n",
        "if 'model' not in globals() or 'tokenizer' not in globals():\n",
        "    print(\"üì• Model not in memory. Reloading from saved adapter...\")\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    # Load base model\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
        "        max_seq_length=2048,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Load the trained adapter\n",
        "    adapter_path = \"/content/drive/My Drive/CyberAgent_Mobile/adapter\"\n",
        "    # Load adapter using PEFT\n",
        "    from peft import PeftModel\n",
        "    model = PeftModel.from_pretrained(model, adapter_path)\n",
        "\n",
        "    print(f\"‚úÖ Model and adapter loaded from {adapter_path}\")\n",
        "\n",
        "print(\"üíæ Preparing model for mobile deployment...\")\n",
        "\n",
        "# Define paths\n",
        "project_path = \"/content/drive/My Drive/CyberAgent_Mobile\"\n",
        "merged_model_path = os.path.join(project_path, \"merged_model\")\n",
        "\n",
        "# Merge the LoRA adapter into the base model\n",
        "# Ensure model is wrapped as PeftModel for merge_and_unload\n",
        "from peft import PeftModel\n",
        "if not isinstance(model, PeftModel):\n",
        "    adapter_path = \"/content/drive/My Drive/CyberAgent_Mobile/adapter\"\n",
        "    print(\"üîÑ Wrapping model with PeftModel for merging...\")\n",
        "    model = PeftModel.from_pretrained(model, adapter_path)\n",
        "print(\"üîÑ Merging LoRA adapter into base model...\")\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "# Clean up PEFT attributes that might cause issues\n",
        "if hasattr(merged_model, 'peft_config'):\n",
        "    delattr(merged_model, 'peft_config')\n",
        "if hasattr(merged_model, '_hf_peft_config_loaded'):\n",
        "    delattr(merged_model, '_hf_peft_config_loaded')\n",
        "# Save the merged model\n",
        "print(f\"üíæ Saving merged model to: {merged_model_path}\")\n",
        "merged_model.save_pretrained(merged_model_path)\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ MODEL SUCCESSFULLY PREPARED FOR MOBILE DEPLOYMENT!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìÅ Location: {merged_model_path}\")\n",
        "print(\"\\nüì± Next Steps for Mobile Deployment:\")\n",
        "print(\"  1. Download the model from Google Drive\")\n",
        "print(\"  2. Convert to mobile format using one of these tools:\")\n",
        "print(\"     ‚Ä¢ PyTorch Mobile (recommended for Android)\")\n",
        "print(\"     ‚Ä¢ ONNX Runtime Mobile\")\n",
        "print(\"     ‚Ä¢ TensorFlow Lite (via ONNX conversion)\")\n",
        "print(\"\\n‚ö° Note: The AI Edge Torch conversion had compatibility issues\")\n",
        "print(\"     with Gemma 2 2B. Use the alternatives above instead.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "iAMVfT5RCcyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*# New Section"
      ],
      "metadata": {
        "id": "CsaepNkJl-n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Convert Model to AI Edge Torch Format\n",
        "import os\n",
        "import torch\n",
        "\n",
        "print(\"üîß Installing AI Edge Torch...\")\n",
        "!pip install -q ai-edge-torch\n",
        "\n",
        "print(\"\\nüì¶ Loading your trained model...\")\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/drive/My Drive/CyberAgent_Mobile/merged_model\"\n",
        "output_path = \"/content/drive/My Drive/CyberAgent_Mobile/ai_edge_model.tflite\"\n",
        "\n",
        "try:\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float32,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"‚úÖ Model loaded successfully\")\n",
        "    print(f\"\\n‚ö†Ô∏è IMPORTANT NOTE:\")\n",
        "    print(\"AI Edge Torch conversion for Gemma 2 2B has known compatibility issues.\")\n",
        "    print(\"This is why your original notebook mentioned the issue.\")\n",
        "    print(\"\\nAttempting conversion anyway...\")\n",
        "\n",
        "    # Import AI Edge Torch\n",
        "    import ai_edge_torch\n",
        "\n",
        "    # Create sample input\n",
        "    sample_input = torch.randint(0, 1000, (1, 128))  # (batch_size, seq_len)\n",
        "\n",
        "    # Attempt conversion\n",
        "    print(\"\\nüîÑ Converting to TFLite format...\")\n",
        "    edge_model = ai_edge_torch.convert(\n",
        "        model.forward,\n",
        "        (sample_input,)\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    edge_model.export(output_path)\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ CONVERSION SUCCESSFUL!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üìÅ Location: {output_path}\")\n",
        "    print(f\"\\nüì≤ Next Steps:\")\n",
        "    print(\"1. Download this file from Google Drive\")\n",
        "    print(\"2. Upload to AI Edge Gallery app or MediaPipe Studio\")\n",
        "    print(\"3. Test your cybersecurity agent!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"‚ùå CONVERSION FAILED (Expected)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Error: {str(e)[:200]}\")\n",
        "    print(\"\\nüîÑ This is the compatibility issue mentioned in the notebook.\")\n",
        "    print(\"\\n‚úÖ ALTERNATIVE SOLUTIONS:\")\n",
        "    print(\"\\n1. Use MediaPipe LLM Inference API (recommended):\")\n",
        "    print(\"   - Supports Gemma models natively\")\n",
        "    print(\"   - Better for production use\")\n",
        "    print(\"   - Guide: https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference\")\n",
        "    print(\"\\n2. Upload to Hugging Face:\")\n",
        "    print(\"   - Your model is already in the right format\")\n",
        "    print(\"   - Can be used with Transformers.js in browser\")\n",
        "    print(\"   - Or with optimum for mobile conversion\")\n",
        "    print(\"\\n3. Build custom Android app:\")\n",
        "    print(\"   - Use ONNX Runtime or PyTorch Mobile\")\n",
        "    print(\"   - Full control over inference\")"
      ],
      "metadata": {
        "id": "m-OCmd1Tmt2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage and Next Steps\n",
        "\n",
        "### Successfully Completed ‚úÖ\n",
        "\n",
        "This notebook has successfully:\n",
        "1. Fine-tuned Gemma 2 2B for cybersecurity actions\n",
        "2. Trained the model with SFT (600 steps) and DPO (150 steps)\n",
        "3. Saved the merged model to Google Drive\n",
        "\n",
        "### Model Location\n",
        "\n",
        "The trained model is available at:\n",
        "- **Google Drive**: `/content/drive/My Drive/CyberAgent_Mobile/merged_model`\n",
        "\n",
        "### Using the Model\n",
        "\n",
        "#### In Android Applications\n",
        "\n",
        "Example: Load the model using PyTorch Mobile or ONNX Runtime\n",
        "\n",
        "#### Model Input/Output Format\n",
        "\n",
        "**Input**: Natural language threat description\n",
        "\n",
        "**Output**: JSON action block\n",
        "\n",
        "### Available Actions\n",
        "\n",
        "The model can output these security actions:\n",
        "- `scan_url(url)`: Check a link for phishing\n",
        "- `kill_process(pid)`: Stop a suspicious app\n",
        "- `isolate_network()`: Cut off internet access\n",
        "- `ignore()`: No threat detected\n",
        "\n",
        "### Notes\n",
        "\n",
        "- **AI Edge Torch conversion** had compatibility issues with Gemma 2 2B. Use PyTorch Mobile or ONNX Runtime instead.\n",
        "- Model size: ~2GB (suitable for modern Android devices with 6GB+ RAM)\n",
        "- Training was optimized with Unsloth for 2x faster performance and 70% less memory"
      ],
      "metadata": {
        "id": "hJuJAn-6elf8"
      }
    }
  ]
}